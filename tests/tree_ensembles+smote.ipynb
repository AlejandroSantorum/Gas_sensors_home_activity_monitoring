{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# <font color='red'><ins> Tree ensembles: Random Forest and Boosting </ins></font>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Smote libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Own libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from preprocessing import *\n",
    "from plotting import *\n",
    "from utils import *"
   ]
  },
  {
   "source": [
    "# 1. Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1. Hyperparameter tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To tune the hyperparameters of our models we are going to use the preprocessed dataset, i.e., the dataset where the stimulus start/end points are corrected.\n",
    "\n",
    "Once, we have the best hyperparameters for a model, we are going to train-test again using only those hyperparameters but with a larger amount of repetitions to get a better generalization error estimation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db_prep = group_datafiles_byID('../datasets/preprocessed/HT_Sensor_prep_metadata.dat', \n",
    "                                  '../datasets/preprocessed/HT_Sensor_prep_dataset.dat')\n",
    "df_db_prep = reclassify_series_samples(df_db_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 300, 500]\n",
    "criterions = ['gini', 'entropy']\n",
    "max_depths = [3, 7, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================\n",
      "N estimators: 100\n",
      "Criterion: gini\n",
      "Max depth: 3\n",
      "Mean accuracy: 0.7946293751841381 +- 0.022340149438788016\n",
      "Mean F1-Score: 0.7314064184956269 +- 0.030436776219018077\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 100\n",
      "Criterion: gini\n",
      "Max depth: 7\n",
      "Mean accuracy: 0.8251705066757214 +- 0.030385405371009255\n",
      "Mean F1-Score: 0.7917698460100201 +- 0.018411437142537727\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 100\n",
      "Criterion: gini\n",
      "Max depth: 11\n",
      "Mean accuracy: 0.776987711616818 +- 0.014463698691850284\n",
      "Mean F1-Score: 0.7329256062850488 +- 0.020797267302252044\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 100\n",
      "Criterion: entropy\n",
      "Max depth: 3\n",
      "Mean accuracy: 0.8466922047650249 +- 0.02000570277513751\n",
      "Mean F1-Score: 0.7974078702927899 +- 0.029569765180974403\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 100\n",
      "Criterion: entropy\n",
      "Max depth: 7\n",
      "Mean accuracy: 0.8627773045945877 +- 0.02029268053720007\n",
      "Mean F1-Score: 0.8392944292939127 +- 0.030397185278727448\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 100\n",
      "Criterion: entropy\n",
      "Max depth: 11\n",
      "Mean accuracy: 0.835719748657015 +- 0.02583066191467667\n",
      "Mean F1-Score: 0.7962037954114768 +- 0.035905305820405374\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 300\n",
      "Criterion: gini\n",
      "Max depth: 3\n",
      "Mean accuracy: 0.8303194221969771 +- 0.03782407206460158\n",
      "Mean F1-Score: 0.7790434462395629 +- 0.05477034571296945\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 300\n",
      "Criterion: gini\n",
      "Max depth: 7\n",
      "Mean accuracy: 0.8524847956458078 +- 0.044428748656727586\n",
      "Mean F1-Score: 0.8136566253859102 +- 0.06379084086869566\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 300\n",
      "Criterion: gini\n",
      "Max depth: 11\n",
      "Mean accuracy: 0.8234801886415912 +- 0.03676513526728777\n",
      "Mean F1-Score: 0.8041293196156266 +- 0.035285602586171506\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 300\n",
      "Criterion: entropy\n",
      "Max depth: 3\n",
      "Mean accuracy: 0.8345316933018813 +- 0.030176442188881703\n",
      "Mean F1-Score: 0.784924351058809 +- 0.03297335365164312\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 300\n",
      "Criterion: entropy\n",
      "Max depth: 7\n",
      "Mean accuracy: 0.8194483838964239 +- 0.027927645026009477\n",
      "Mean F1-Score: 0.7772024191748015 +- 0.032832754869409446\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 300\n",
      "Criterion: entropy\n",
      "Max depth: 11\n",
      "Mean accuracy: 0.7984585737751763 +- 0.044820764558831765\n",
      "Mean F1-Score: 0.7578822552605845 +- 0.056578553445007194\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 500\n",
      "Criterion: gini\n",
      "Max depth: 3\n",
      "Mean accuracy: 0.8488478218138876 +- 0.02085520649676731\n",
      "Mean F1-Score: 0.809578666392937 +- 0.02548976658213261\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 500\n",
      "Criterion: gini\n",
      "Max depth: 7\n",
      "Mean accuracy: 0.8039276434122922 +- 0.03061587646989114\n",
      "Mean F1-Score: 0.752986509665322 +- 0.03252196839048643\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 500\n",
      "Criterion: gini\n",
      "Max depth: 11\n",
      "Mean accuracy: 0.8353002413475772 +- 0.02229085170012069\n",
      "Mean F1-Score: 0.7891932175479696 +- 0.02796902587061161\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 500\n",
      "Criterion: entropy\n",
      "Max depth: 3\n",
      "Mean accuracy: 0.8560749782004966 +- 0.02365187259992991\n",
      "Mean F1-Score: 0.8144363597109973 +- 0.03421527740732374\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 500\n",
      "Criterion: entropy\n",
      "Max depth: 7\n",
      "Mean accuracy: 0.8375918830651056 +- 0.028614489329118094\n",
      "Mean F1-Score: 0.7924384362550532 +- 0.040627209365423474\n",
      "============================================\n",
      "============================================\n",
      "N estimators: 500\n",
      "Criterion: entropy\n",
      "Max depth: 11\n",
      "Mean accuracy: 0.7791410241130033 +- 0.039248333143512365\n",
      "Mean F1-Score: 0.728351993011039 +- 0.053286176331846555\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "VAL_REPS = 3\n",
    "\n",
    "for nest in n_estimators:\n",
    "    for crit in criterions:\n",
    "        for depth in max_depths:\n",
    "            # Mean error arrays\n",
    "            errs_acc = []\n",
    "            errs_f1 = []\n",
    "            for i in range(VAL_REPS):\n",
    "                # Splitting set in train (80%) and test set\n",
    "                df_train, df_test = split_series_byID(0.8, df_db_prep)\n",
    "                xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "                xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "                # Init clf\n",
    "                rfc = RandomForestClassifier(n_estimators=nest, criterion=crit, max_depth=depth, n_jobs=-1)\n",
    "                # Train clf on train set\n",
    "                rfc.fit(xtrain, ytrain)\n",
    "                # Getting clf metrics on test set\n",
    "                y_pred = rfc.predict(xtest)\n",
    "                acc = accuracy_score(ytest, y_pred)\n",
    "                f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "                errs_acc.append(acc)\n",
    "                errs_f1.append(f1)\n",
    "\n",
    "            errs_acc = np.asarray(errs_acc)\n",
    "            errs_f1 = np.asarray(errs_f1)\n",
    "            print('============================================')\n",
    "            print('N estimators:', nest)\n",
    "            print('Criterion:', crit)\n",
    "            print('Max depth:', depth)\n",
    "            print('Mean accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "            print('Mean F1-Score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "            print('============================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 1.2 Performance in preprocessed dataset with best hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Repetition  0 done\n",
      "Repetition  1 done\n",
      "Repetition  2 done\n",
      "Repetition  3 done\n",
      "Repetition  4 done\n",
      "==> Mean accuracy: 0.8590514681255197 +- 0.038124397895361636\n",
      "==> Mean F1-Score: 0.8216484752586943 +- 0.04936510907701275\n"
     ]
    }
   ],
   "source": [
    "REPS = 5\n",
    "\n",
    "# Mean error arrays\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "for i in range(REPS):\n",
    "    # Splitting set in train (80%) and test set\n",
    "    df_train, df_test = split_series_byID(0.8, df_db_prep)\n",
    "    xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "    xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "    # Init clf\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    # Train clf on train set\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    # Getting clf metrics on test set\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "    print('Repetition ', i, 'done')\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "print('==> Mean accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('==> Mean F1-Score:', errs_f1.mean(), '+-', errs_f1.std())"
   ]
  },
  {
   "source": [
    "## 1.3 Performance in moving windows dataset with best hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/preprocessed/window120_dataset.pkl', 'rb') as f: \n",
    "    df_db_win = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Repetition 1 done\n",
      "Repetition 2 done\n",
      "Repetition 3 done\n",
      "Repetition 4 done\n",
      "Repetition 5 done\n",
      "==> Mean accuracy: 0.8621029966033891 +- 0.016928431408150535\n",
      "==> Mean F1-Score: 0.8283254526750359 +- 0.022128326408941596\n"
     ]
    }
   ],
   "source": [
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity',\n",
    "            'R1_mean', 'R2_mean', 'R3_mean', 'R4_mean', 'R5_mean', 'R6_mean', 'R7_mean',\n",
    "            'R8_mean', 'Temp._mean', 'Humidity_mean', 'R1_std', 'R2_std', 'R3_std', 'R4_std',\n",
    "            'R5_std', 'R6_std', 'R7_std', 'R8_std', 'Temp._std', 'Humidity_std']\n",
    "\n",
    "REPS = 5\n",
    "\n",
    "# Mean error arrays\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "for i in range(REPS):\n",
    "    # Splitting set in train (80%) and test set\n",
    "    df_train, df_test = split_series_byID(0.8, df_db_win)\n",
    "    #df_train, df_test = norm_train_test(df_train, df_test, features)\n",
    "    xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "    xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "    # Init clf\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    # Train clf on train set\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    # Getting clf metrics on test set\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "    print('Repetition', i+1, 'done')\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "print('==> Mean accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('==> Mean F1-Score:', errs_f1.mean(), '+-', errs_f1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 1.4 Performance normalizing and using SMOTE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "In the next cell we are going to execute **several Random Forest classifiers**.\n",
    "\n",
    "The datasets used for training are:\n",
    "- *Raw dataset*: initial dataset of the original authors article/repository.\n",
    "- *Preprocessed dataset*: higher quality dataset, where certain values have been corrected (stimulus start/end points).\n",
    "- *Moving windows dataset*: dataset where several features have been added to capture the last 120 samples mean and std (moving average + moving std).\n",
    "\n",
    "For all these datasets, **normalization has been tested**, as weel as the **Synthetic Minority Oversample Technique (SMOTE) for imbalanced data**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== Raw not norm ====\n",
      "Accuracy: 0.8418960522995799 +- 0.03632088039392034\n",
      "f1-score: 0.7971172515127904 +- 0.04712454772292015\n",
      "Accuracy (smote): 0.8277699338947454 +- 0.03764657496658198\n",
      "f1-score (smote): 0.8082176890316616 +- 0.041986899096288714\n",
      "==== Raw normalized ====\n",
      "Accuracy: 0.8435238958626406 +- 0.016988139870408495\n",
      "f1-score: 0.8044838931123677 +- 0.02726273359176542\n",
      "Accuracy (smote): 0.8352368865135366 +- 0.029350892504258912\n",
      "f1-score (smote): 0.8167438646556081 +- 0.03198988701165813\n",
      "==== Prep not norm ====\n",
      "Accuracy: 0.8065367076851159 +- 0.029776930251357886\n",
      "f1-score: 0.7572592024620618 +- 0.03996370131297966\n",
      "Accuracy (smote): 0.7907933461773212 +- 0.029474943405134558\n",
      "f1-score (smote): 0.7667767335193576 +- 0.03574504446815725\n",
      "==== Prep normalized ====\n",
      "Accuracy: 0.8518799497840609 +- 0.046531772282859415\n",
      "f1-score: 0.8152035473309679 +- 0.06113266960072451\n",
      "Accuracy (smote): 0.8308586053612501 +- 0.05510826568942438\n",
      "f1-score (smote): 0.816378820351669 +- 0.057696944279277226\n",
      "==== Windows 120 not norm ====\n",
      "Accuracy: 0.8529863048507637 +- 0.02038014310049991\n",
      "f1-score: 0.8181126667592651 +- 0.02757802983341933\n",
      "Accuracy (smote): 0.8445146762730947 +- 0.015484321339345592\n",
      "f1-score (smote): 0.8336477399298723 +- 0.01791889432209128\n",
      "==== Windows 120 normalized ====\n",
      "Accuracy: 0.8646819675672726 +- 0.03289853992666121\n",
      "f1-score: 0.8308314515317747 +- 0.04477833393157427\n",
      "Accuracy (smote): 0.8556395368175896 +- 0.03131285990802882\n",
      "f1-score (smote): 0.8452049363445511 +- 0.03246396723179524\n"
     ]
    }
   ],
   "source": [
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity']\n",
    "\n",
    "########################################################################################\n",
    "#   RAW DATASET\n",
    "#       Normalized and not normalized: using SMOTE in both cases\n",
    "#########################################################################################\n",
    "df_db_raw = group_datafiles_byID('../datasets/raw/HT_Sensor_metadata.dat', \n",
    "                             '../datasets/raw/HT_Sensor_dataset.dat')\n",
    "df_db_raw = reclassify_series_samples(df_db_raw)\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "# Oversampling and undersampling dictionary\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING WITHOUT NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset and splitting\n",
    "    df_train_raw, df_test_raw = split_series_byID(0.8, df_db_raw)\n",
    "    xtrain, ytrain = df_train_raw[features].values, df_train_raw['class'].values\n",
    "    xtest, ytest = df_test_raw[features].values, df_test_raw['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Raw not norm ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset, splitting and normalizing\n",
    "    df_train_raw, df_test_raw = split_series_byID(0.8, df_db_raw)\n",
    "    df_train_raw, df_test_raw = norm_train_test(df_train_raw, df_test_raw, features)\n",
    "    xtrain, ytrain = df_train_raw[features].values, df_train_raw['class'].values\n",
    "    xtest, ytest = df_test_raw[features].values, df_test_raw['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Raw normalized ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#   PREPROCESSED DATASET\n",
    "#       Normalized and not normalized: using SMOTE in both cases\n",
    "#########################################################################################\n",
    "df_db_prep = group_datafiles_byID('../datasets/preprocessed/HT_Sensor_prep_metadata.dat', \n",
    "                             '../datasets/preprocessed/HT_Sensor_prep_dataset.dat')\n",
    "df_db_prep = reclassify_series_samples(df_db_prep)\n",
    "\n",
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity']\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING WITHOUT NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset and splitting\n",
    "    df_train_prep, df_test_prep = split_series_byID(0.8, df_db_prep)\n",
    "    xtrain, ytrain = df_train_prep[features].values, df_train_prep['class'].values\n",
    "    xtest, ytest = df_test_prep[features].values, df_test_prep['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Prep not norm ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset, splitting and normalizing\n",
    "    df_train_prep, df_test_prep = split_series_byID(0.8, df_db_prep)\n",
    "    df_train_prep, df_test_prep = norm_train_test(df_train_prep, df_test_prep, features)\n",
    "    xtrain, ytrain = df_train_prep[features].values, df_train_prep['class'].values\n",
    "    xtest, ytest = df_test_prep[features].values, df_test_prep['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Prep normalized ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#   PREPROCESSED DATASET with MOVING WINDOWS\n",
    "#       Normalized and not normalized: using SMOTE in both cases\n",
    "#########################################################################################\n",
    "with open('../datasets/preprocessed/window120_dataset.pkl', 'rb') as f: \n",
    "    df_db = pickle.load(f)\n",
    "\n",
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity',\n",
    "            'R1_mean', 'R2_mean', 'R3_mean', 'R4_mean', 'R5_mean', 'R6_mean', 'R7_mean',\n",
    "            'R8_mean', 'Temp._mean', 'Humidity_mean', 'R1_std', 'R2_std', 'R3_std', 'R4_std',\n",
    "            'R5_std', 'R6_std', 'R7_std', 'R8_std', 'Temp._std', 'Humidity_std']\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING WITHOUT NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset and splitting\n",
    "    df_train, df_test = split_series_byID(0.8, df_db)\n",
    "    xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "    xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Windows 120 not norm ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity',\n",
    "            'R1_mean', 'R2_mean', 'R3_mean', 'R4_mean', 'R5_mean', 'R6_mean', 'R7_mean',\n",
    "            'R8_mean', 'Temp._mean', 'Humidity_mean', 'R1_std', 'R2_std', 'R3_std', 'R4_std',\n",
    "            'R5_std', 'R6_std', 'R7_std', 'R8_std', 'Temp._std', 'Humidity_std']\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset, splitting and normalizing\n",
    "    df_train, df_test = split_series_byID(0.8, df_db)\n",
    "    df_train, df_test = norm_train_test(df_train, df_test, features)\n",
    "    xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "    xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Windows 120 normalized ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 2. Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}