{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tree ensembles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Smote libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Own libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from preprocessing import *\n",
    "from plotting import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Ensembles of decision trees: Random Forest\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In the next cell we are going to execute **several Random Forest classifiers**.\n",
    "\n",
    "The datasets used for training are:\n",
    "- *Raw dataset*: initial dataset of the original authors article/repository.\n",
    "- *Preprocessed dataset*: higher quality dataset, where certain values have been corrected (stimulus start/end points).\n",
    "- *Moving windows dataset*: dataset where several features have been added to capture the last 120 samples mean and std (moving average + moving std).\n",
    "\n",
    "For all these datasets, **normalization has been tested**, as weel as the **Synthetic Minority Oversample Technique (SMOTE) for imbalanced data**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== Raw not norm ====\n",
      "Accuracy: 0.8418960522995799 +- 0.03632088039392034\n",
      "f1-score: 0.7971172515127904 +- 0.04712454772292015\n",
      "Accuracy (smote): 0.8277699338947454 +- 0.03764657496658198\n",
      "f1-score (smote): 0.8082176890316616 +- 0.041986899096288714\n",
      "==== Raw normalized ====\n",
      "Accuracy: 0.8435238958626406 +- 0.016988139870408495\n",
      "f1-score: 0.8044838931123677 +- 0.02726273359176542\n",
      "Accuracy (smote): 0.8352368865135366 +- 0.029350892504258912\n",
      "f1-score (smote): 0.8167438646556081 +- 0.03198988701165813\n",
      "==== Prep not norm ====\n",
      "Accuracy: 0.8065367076851159 +- 0.029776930251357886\n",
      "f1-score: 0.7572592024620618 +- 0.03996370131297966\n",
      "Accuracy (smote): 0.7907933461773212 +- 0.029474943405134558\n",
      "f1-score (smote): 0.7667767335193576 +- 0.03574504446815725\n",
      "==== Prep normalized ====\n",
      "Accuracy: 0.8518799497840609 +- 0.046531772282859415\n",
      "f1-score: 0.8152035473309679 +- 0.06113266960072451\n",
      "Accuracy (smote): 0.8308586053612501 +- 0.05510826568942438\n",
      "f1-score (smote): 0.816378820351669 +- 0.057696944279277226\n",
      "==== Windows 120 not norm ====\n",
      "Accuracy: 0.8529863048507637 +- 0.02038014310049991\n",
      "f1-score: 0.8181126667592651 +- 0.02757802983341933\n",
      "Accuracy (smote): 0.8445146762730947 +- 0.015484321339345592\n",
      "f1-score (smote): 0.8336477399298723 +- 0.01791889432209128\n",
      "==== Windows 120 normalized ====\n",
      "Accuracy: 0.8646819675672726 +- 0.03289853992666121\n",
      "f1-score: 0.8308314515317747 +- 0.04477833393157427\n",
      "Accuracy (smote): 0.8556395368175896 +- 0.03131285990802882\n",
      "f1-score (smote): 0.8452049363445511 +- 0.03246396723179524\n"
     ]
    }
   ],
   "source": [
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity']\n",
    "\n",
    "########################################################################################\n",
    "#   RAW DATASET\n",
    "#       Normalized and not normalized: using SMOTE in both cases\n",
    "#########################################################################################\n",
    "df_db_raw = group_datafiles_byID('../datasets/raw/HT_Sensor_metadata.dat', \n",
    "                             '../datasets/raw/HT_Sensor_dataset.dat')\n",
    "df_db_raw = reclassify_series_samples(df_db_raw)\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "# Oversampling and undersampling dictionary\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING WITHOUT NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset and splitting\n",
    "    df_train_raw, df_test_raw = split_series_byID(0.8, df_db_raw)\n",
    "    xtrain, ytrain = df_train_raw[features].values, df_train_raw['class'].values\n",
    "    xtest, ytest = df_test_raw[features].values, df_test_raw['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Raw not norm ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset, splitting and normalizing\n",
    "    df_train_raw, df_test_raw = split_series_byID(0.8, df_db_raw)\n",
    "    df_train_raw, df_test_raw = norm_train_test(df_train_raw, df_test_raw, features)\n",
    "    xtrain, ytrain = df_train_raw[features].values, df_train_raw['class'].values\n",
    "    xtest, ytest = df_test_raw[features].values, df_test_raw['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=6, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Raw normalized ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#   PREPROCESSED DATASET\n",
    "#       Normalized and not normalized: using SMOTE in both cases\n",
    "#########################################################################################\n",
    "df_db_prep = group_datafiles_byID('../datasets/preprocessed/HT_Sensor_prep_metadata.dat', \n",
    "                             '../datasets/preprocessed/HT_Sensor_prep_dataset.dat')\n",
    "df_db_prep = reclassify_series_samples(df_db_prep)\n",
    "\n",
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity']\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING WITHOUT NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset and splitting\n",
    "    df_train_prep, df_test_prep = split_series_byID(0.8, df_db_prep)\n",
    "    xtrain, ytrain = df_train_prep[features].values, df_train_prep['class'].values\n",
    "    xtest, ytest = df_test_prep[features].values, df_test_prep['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Prep not norm ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset, splitting and normalizing\n",
    "    df_train_prep, df_test_prep = split_series_byID(0.8, df_db_prep)\n",
    "    df_train_prep, df_test_prep = norm_train_test(df_train_prep, df_test_prep, features)\n",
    "    xtrain, ytrain = df_train_prep[features].values, df_train_prep['class'].values\n",
    "    xtest, ytest = df_test_prep[features].values, df_test_prep['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Prep normalized ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#   PREPROCESSED DATASET with MOVING WINDOWS\n",
    "#       Normalized and not normalized: using SMOTE in both cases\n",
    "#########################################################################################\n",
    "with open('../datasets/preprocessed/window120_dataset.pkl', 'rb') as f: \n",
    "    df_db = pickle.load(f)\n",
    "\n",
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity',\n",
    "            'R1_mean', 'R2_mean', 'R3_mean', 'R4_mean', 'R5_mean', 'R6_mean', 'R7_mean',\n",
    "            'R8_mean', 'Temp._mean', 'Humidity_mean', 'R1_std', 'R2_std', 'R3_std', 'R4_std',\n",
    "            'R5_std', 'R6_std', 'R7_std', 'R8_std', 'Temp._std', 'Humidity_std']\n",
    "\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING WITHOUT NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset and splitting\n",
    "    df_train, df_test = split_series_byID(0.8, df_db)\n",
    "    xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "    xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Windows 120 not norm ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())\n",
    "\n",
    "\n",
    "features = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'Temp.', 'Humidity',\n",
    "            'R1_mean', 'R2_mean', 'R3_mean', 'R4_mean', 'R5_mean', 'R6_mean', 'R7_mean',\n",
    "            'R8_mean', 'Temp._mean', 'Humidity_mean', 'R1_std', 'R2_std', 'R3_std', 'R4_std',\n",
    "            'R5_std', 'R6_std', 'R7_std', 'R8_std', 'Temp._std', 'Humidity_std']\n",
    "errs_acc = []\n",
    "errs_f1 = []\n",
    "errs_acc_smote = []\n",
    "errs_f1_smote = []\n",
    "\n",
    "over_dict = {'banana': 175000, 'wine': 175000}\n",
    "under_dict = {'background': 500000}\n",
    "\n",
    "# TRAINING NORMALIZING\n",
    "for i in range(7):\n",
    "    # Reading dataset, splitting and normalizing\n",
    "    df_train, df_test = split_series_byID(0.8, df_db)\n",
    "    df_train, df_test = norm_train_test(df_train, df_test, features)\n",
    "    xtrain, ytrain = df_train[features].values, df_train['class'].values\n",
    "    xtest, ytest = df_test[features].values, df_test['class'].values\n",
    "\n",
    "    # Training without using SMOTE\n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc.append(acc)\n",
    "    errs_f1.append(f1)\n",
    "\n",
    "    # Training using SMOTE\n",
    "    oversample = SMOTE(sampling_strategy=over_dict)\n",
    "    undersample = RandomUnderSampler(sampling_strategy=under_dict)\n",
    "    xtrain, ytrain = oversample.fit_resample(xtrain, ytrain)\n",
    "    xtrain, ytrain = undersample.fit_resample(xtrain, ytrain)   \n",
    "    rfc = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=7, n_jobs=-1)\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    y_pred = rfc.predict(xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    f1 = f1_score(ytest, y_pred, average='weighted')\n",
    "    errs_acc_smote.append(acc)\n",
    "    errs_f1_smote.append(f1)\n",
    "\n",
    "errs_acc = np.asarray(errs_acc)\n",
    "errs_f1 = np.asarray(errs_f1)\n",
    "errs_acc_smote = np.asarray(errs_acc_smote)\n",
    "errs_f1_smote = np.asarray(errs_f1_smote)\n",
    "print('==== Windows 120 normalized ====')\n",
    "print('Accuracy:', errs_acc.mean(), '+-', errs_acc.std())\n",
    "print('f1-score:', errs_f1.mean(), '+-', errs_f1.std())\n",
    "print('Accuracy (smote):', errs_acc_smote.mean(), '+-', errs_acc_smote.std())\n",
    "print('f1-score (smote):', errs_f1_smote.mean(), '+-', errs_f1_smote.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}